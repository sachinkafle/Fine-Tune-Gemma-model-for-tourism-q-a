Model: Gemma (a powerful open-source language model by Google)
Fine-Tuning Method: PEFT (Parameter-Efficient Fine-Tuning)
Optimization Techniques: LoRA (Low-Rank Adaptation) / QLoRA / Prefix Tuning / Adapters
Task-Specific Adaptation: Improved performance in [question-answering, chatbot, summarization, etc.]
Computational Efficiency: Reduces training cost and memory usage compared to full fine-tuning
Frameworks & Tools:
Hugging Face's transformers and peft libraries
PyTorch / TensorFlow
Accelerated training with GPU/TPU
